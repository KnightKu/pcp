About:
------
PCP: a parallel copy program for lustre.

Latest Version:
---------------

The latest version of pcp can be downloaded from github:

https://github.com/wtsi-ssg/pcp


Requirements:
-------------
pcp requires the following:

mpi4py 
http://mpi4py.scipy.org/

Lustre C api library, liblustreapi.so or liblustreapi.a


Installation
------------

Install pcp using the usual distutils method:

python setup.py build
python setup.py install #  usually requires root access

If running as a non priviliged user, you can install in your home directory 
by running:

python setup.py install --user

If you have the lustreapi libraries install in a non-standard location, you
can use the following option to setup.py 

--with-liblustre=/path/to/library


liblustreapi
------------

lustre clients prior to v 2.3 shipped with a static version of liblustreapi 
rather than a dynamic one. pcp requires a dynamic library. The build process 
will look for liblustreapi.so first.  If it does not find one, it will look for
liblustreapi.a and then convert it into a shared library. The shared library
will be installed alongside the python module.

Manual liblustreapi installation
--------------------------------

If setup.py is unable to convert the library automatically, you can try doing
it by hand. The procedure to convert the library is something along these 
lines:

ar -x liblustreapi.a
gcc -shared -o lublustreapi.so *.o

Place the resulting library in the /lustre directory in the source tree, and 
then re-run the setup.py build / install step.


Usage
-----

pcp is similar to cp -r ; simply give it a source directory and destination
and pcp will recursively copy the source directory to the destination in
parallel. 

pcp has a number of useful options; use pcp -h to see a description.

Checksum
--------

If run with the -c flag, pcp will md5 checksum files after they are copied.
pcp tries, but does guarantee, to run the md5sum on a separate rank to that
which did the copy. 

lustre striping
---------------

If run with the -l flag, pcp will be lustre stripe aware. When it encounters
a striped file it will stripe the copy across all OSTs at the destination. Note
that it does not exactly preserve stripe information, but copes with case where
the number of OSTs is different at the source and destination.

If a size is specified with -ls, pcp will not stripe files smaller than this,
even if the original is striped.

If -lf is set, pcp will stripe all files at the destination. This options can
be combined with -ls to force striping of files over a certain size.

Checkpointing
-------------

pcp support serveral checkpointing methods. The checkpoint allows a copy,
interrupted for any reason, to be restarted. Files which were in the process of
being copied will be recopied in their entirey.

If you specify a dumpfile with -K, a checkpoint will be written every 60 minutes.
The checkpoint period can be varied with the -Km option. If the program runs into
an exception, it will also write out a checkpoint before it exits.

Alternatively, pcp can be made to generate a checkpoint by sending it SIGUSR1.
If no dumpfile has been specified with -K, it will default to writing out to
"pcp_checkpoint.db" in the program's current working directory.

To resume from checkpoint, start pcp with the -R <dumpfile> option. All pcp command
line parameters will be taken from the dumpfile; any other command line arguments you
give to pcp will be ignored.

Note that you can safely resume a checkpoint with a different number of processes than
the original.


Other Useful Options
--------------------

A dead worker timeout can be specified with -d; if workers do not respond within
timeout seconds of the job starting, the job will terminate.

If run with -p, pcp will attempt to preserve the ownership, permissions and 
timestamps of the copied files.


Invocation
----------

pcp should be invoked by mpirun, and needs at least 2 tasks to run correctly.

For maximum efficiency, ensure tasks are spread across the cluster to maximise
network bandwidth to the filesystem. On most systems that means spreading tasks
across as many machines as possible, rather than packing them together on as 
few hosts as possible.

Consult your queuing system and local MPI documentation for the 
appropriate commands to achieve this.

Example LSF bsub distributing jobs across as many hosts as possible:

bsub -R "span[ptile=1]" -oo logfile.txt -n 4  mpirun pcp ... 
